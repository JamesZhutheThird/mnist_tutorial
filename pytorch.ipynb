{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch is a popular deep learning framework and it's easy to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6.0\n",
      "0.7.0\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)\n",
    "print(device)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we read the mnist data, preprocess them and encapsulate them into dataloader form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 10000\n"
     ]
    }
   ],
   "source": [
    "# preprocessing\n",
    "normalize = transforms.Normalize(mean=[.5], std=[.5])\n",
    "transform = transforms.Compose([transforms.ToTensor(), normalize])\n",
    "\n",
    "# download and load the data\n",
    "train_dataset = torchvision.datasets.MNIST(root='./mnist/', train=True, transform=transform, download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./mnist/', train=False, transform=transform, download=False)\n",
    "\n",
    "# encapsulate them into dataloader form\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "test_loader = data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=True) \n",
    "\n",
    "print(len(train_dataset), len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we define the model, object function and optimizer that we use to classify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleNet(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): Sigmoid()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Dropout(p=0.15, inplace=False)\n",
      "    (4): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (5): Sigmoid()\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Dropout(p=0.15, inplace=False)\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=120, bias=True)\n",
      "    (1): Sigmoid()\n",
      "    (2): Linear(in_features=120, out_features=84, bias=True)\n",
      "    (3): Sigmoid()\n",
      "    (4): Linear(in_features=84, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "# TODO:define model\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, 5), # in_channels, out_channels, kernel_size\n",
    "            nn.Sigmoid(),\n",
    "            nn.MaxPool2d(2, 2), # kernel_size, stride\n",
    "            nn.Dropout(0.15),    # drop_prob\n",
    "            nn.Conv2d(6, 16, 5),\n",
    "            nn.Sigmoid(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout(0.15)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256, 120),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(84, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        feature = self.conv(img)\n",
    "        output = self.fc(feature.view(img.shape[0], -1))\n",
    "        return output\n",
    "    \n",
    "model = SimpleNet()\n",
    "\n",
    "# TODO:define loss function and optimiter\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n",
    "\n",
    "# initialize parameters\n",
    "for param in model.parameters():\n",
    "    nn.init.normal_(param, mean=0, std=0.01) \n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can start to train and evaluate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:12<00:00, 38.76it/s]\n",
      "100%|██████████| 468/468 [00:07<00:00, 58.80it/s]\n",
      "100%|██████████| 78/78 [00:01<00:00, 59.49it/s]\n",
      "  1%|          | 4/468 [00:00<00:12, 36.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 2.2233, train acc 0.263, test acc 0.264, time 21.3 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:12<00:00, 38.14it/s]\n",
      "100%|██████████| 468/468 [00:07<00:00, 59.08it/s]\n",
      "100%|██████████| 78/78 [00:01<00:00, 59.49it/s]\n",
      "  1%|          | 4/468 [00:00<00:12, 37.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, loss 1.6163, train acc 0.627, test acc 0.623, time 21.5 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:12<00:00, 37.99it/s]\n",
      "100%|██████████| 468/468 [00:08<00:00, 58.44it/s]\n",
      "100%|██████████| 78/78 [00:01<00:00, 56.23it/s]\n",
      "  1%|          | 4/468 [00:00<00:12, 37.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3, loss 0.7848, train acc 0.904, test acc 0.906, time 21.7 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:12<00:00, 37.19it/s]\n",
      "100%|██████████| 468/468 [00:08<00:00, 57.62it/s]\n",
      "100%|██████████| 78/78 [00:01<00:00, 59.88it/s]\n",
      "  1%|          | 5/468 [00:00<00:11, 41.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4, loss 0.2831, train acc 0.955, test acc 0.957, time 22.0 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:12<00:00, 37.64it/s]\n",
      "100%|██████████| 468/468 [00:08<00:00, 58.41it/s]\n",
      "100%|██████████| 78/78 [00:01<00:00, 58.97it/s]\n",
      "  1%|          | 4/468 [00:00<00:12, 37.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5, loss 0.1859, train acc 0.963, test acc 0.964, time 21.8 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:12<00:00, 37.69it/s]\n",
      "100%|██████████| 468/468 [00:08<00:00, 58.18it/s]\n",
      "100%|██████████| 78/78 [00:01<00:00, 58.16it/s]\n",
      "  1%|          | 4/468 [00:00<00:11, 39.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6, loss 0.1513, train acc 0.969, test acc 0.971, time 21.8 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:12<00:00, 37.94it/s]\n",
      "100%|██████████| 468/468 [00:07<00:00, 58.65it/s]\n",
      "100%|██████████| 78/78 [00:01<00:00, 59.17it/s]\n",
      "  1%|          | 4/468 [00:00<00:11, 39.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7, loss 0.1314, train acc 0.975, test acc 0.975, time 21.6 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:12<00:00, 38.35it/s]\n",
      "100%|██████████| 468/468 [00:07<00:00, 58.61it/s]\n",
      "100%|██████████| 78/78 [00:01<00:00, 58.97it/s]\n",
      "  1%|          | 5/468 [00:00<00:11, 39.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8, loss 0.1189, train acc 0.977, test acc 0.977, time 21.5 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:12<00:00, 37.89it/s]\n",
      "100%|██████████| 468/468 [00:08<00:00, 58.35it/s]\n",
      "100%|██████████| 78/78 [00:01<00:00, 57.81it/s]\n",
      "  1%|          | 4/468 [00:00<00:11, 38.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9, loss 0.1087, train acc 0.980, test acc 0.979, time 21.7 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 468/468 [00:12<00:00, 38.12it/s]\n",
      "100%|██████████| 468/468 [00:08<00:00, 58.42it/s]\n",
      "100%|██████████| 78/78 [00:01<00:00, 59.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10, loss 0.1020, train acc 0.981, test acc 0.979, time 21.6 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# train and evaluate\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    train_l_sum,  batch_count, start = 0.0, 0, time.time()\n",
    "    \n",
    "    model.train() # 训练模式\n",
    " \n",
    "    for images, labels in tqdm(train_loader):\n",
    "        \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        labels_hat = model(images)\n",
    "        l = criterion(labels_hat, labels)\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        train_l_sum += l.cpu().item()\n",
    "        batch_count += 1\n",
    "\n",
    "    train_acc_sum, train_n, test_acc_sum, test_n = 0.0, 0, 0.0, 0\n",
    "\n",
    "    model.eval() # 评估模式, 这会关闭dropout\n",
    "    \n",
    "    for images, labels in tqdm(train_loader):\n",
    "        train_acc_sum += (model(images.to(device)).argmax(dim=1) == labels.to(device)).float().sum().cpu().item()\n",
    "        train_n += labels.shape[0]\n",
    "        \n",
    "    for images, labels in tqdm(test_loader):\n",
    "        test_acc_sum += (model(images.to(device)).argmax(dim=1) == labels.to(device)).float().sum().cpu().item()\n",
    "        test_n += labels.shape[0]\n",
    "    \n",
    "    print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'\n",
    "          % (epoch + 1, train_l_sum / batch_count, train_acc_sum / train_n, test_acc_sum / test_n, time.time() - start))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q5:\n",
    "Please print the training and testing accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 98.13%\n",
      "Testing accuracy: 97.92%\n"
     ]
    }
   ],
   "source": [
    "train_accuracy=train_acc_sum / train_n\n",
    "test_accuracy=test_acc_sum / test_n\n",
    "print('Training accuracy: %0.2f%%' % (train_accuracy*100))\n",
    "print('Testing accuracy: %0.2f%%' % (test_accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "|     | Training accuracy(%) | Testing accuracy(%) |\n",
    "|:---:|:--------------------:|:-------------------:|\n",
    "|  Q5 |         98.13        |        97.92        |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
